---
title: "Data Science עם R - Linear and Logistic Regression"
author: "עדי שריד / adi@sarid-ins.co.il"
output: html_document
---
```{css, echo=FALSE}
p, h1, h2, h3, h4, ul, ol {
  direction: rtl;
}
```

בפרק הקודם הזכרנו שבחיזוי סטטיסטי אנחנו מחפשים את הפונקציה $f$ שמביאה לתוצאת החיזוי הטובה ביותר:

\[Y=f(X)+\epsilon\]

רגרסיה לינארית היא שיטה סטטיסטית המניחה מבנה פשטני מאוד, אך שימושי להפליא. גם אם רוב התופעות הנצפות אינן "לינאריות", בהרבה מהמקרים המעשיים רגרסיה לינארית תביא תוצאות טובות, או לפחות תעזור לנו לחפש בכיוונים טובים, ולהבין אילו משתנים משפיעים על משתנה המטרה שלנו ובאיזה כיוון.

מודל הרגרסיה הלינארית מניח שהפונקציה $f$ היא לינארית, כלומר:

\[Y = \beta_0 + \beta_1x_1+\ldots+\beta_px_p\ + \epsilon\]

כדי לחשב את המקדמים $\{\beta_i\}_{i=0}^p$, אנחנו פותרים בעיית מינימום - המקדמים שמביאים את השגיאה הריבועית למינימום.

```{r linear regression explanation, message=FALSE, warning=FALSE}
library(tidyverse)

iris_setosa <- iris %>%
  filter(Species == "setosa") %>%
  mutate(Sepal.Length = jitter(Sepal.Length))

setosa_lm = lm(data = iris_setosa,
               formula = Sepal.Width ~ Sepal.Length)

iris_lm_errors <- iris_setosa %>%
  mutate(Sepal.Width.pred = predict(setosa_lm, 
                                    newdata = iris %>% 
                                      filter(Species == "setosa")))

ggplot(iris_lm_errors, 
       aes(x=Sepal.Length, y=Sepal.Width)) + 
  geom_point() + stat_smooth(method = "lm", se = FALSE) + 
  geom_segment(aes(x = Sepal.Length, xend = Sepal.Length, y = Sepal.Width, yend = Sepal.Width.pred))

```

פתרון הרגרסיה הלינארית מביא לקו רגרסיה הממזער את המרחקים שבין הנקודות (התצפיות) לבינו (ריבועי המרחקים שבתרשים).

כל מודל רגרסיה מכיל סיכום כך לדוגמה מודל רגרסיה מורכב יותר עבור אותה בעיה (רק המכיל את כל המשתנים) יראה כך:

```{r iris more complex lm}

iris_lm_complete <- lm(data = iris, 
                       formula = Sepal.Width ~ Sepal.Length + Petal.Width + Petal.Length)

summary(iris_lm_complete)
  
```

שימו לב לשימוש בפרמטר formula שבפקודה `lm`. פרמטר זה מתאר את המודל לפיו הפקודה צריכה לבנות את הרגרסיה.
התוצר של הפקודה `summary` מפרט את המודל, בעמודה estimate ניתן לראות את המקדמים למשתנים. עמודה Std error מציגה את סטיית התקן, ושתי העמודות האחרונות מתייחסות למבחן סטטיסטי לגבי המקדם הרלוונטי (האם המקדם שונה מ-0 באופן מובהק סטטיסטי). הכוכביות שבימין כל שורה מציגות את רמת המובהקות על פי מדרגות. במדעי החברה מקובל להתייחס לרמות מובהקות מתחת ל-0.05 כמשמעותיות. המודל בדוגמה יתורגם כך:

\[\text{Sepal.Width} \approx 1.04 + 0.61\cdot\text{Sepal.Length} + 0.56\cdot\text{Petal.Width} -0.58\cdot\text{Petal.Length}\]

בתחתית הדיווח מופיע המדד Multiple R-squared ו-Adjusted R-sqaured. שניהם מדדים הנוגעים לטיב הרגרסיה. אלו מדדים שנעים בין 0-1, וכאשר הם קרובים ל-1 המשמעות היא שהרגרסיה מצליחה להסביר חלק משמעותי מהשונות שבמשתנה התלוי.

מדד נוסף למידת הדיוק של הרגרסיה הוא ה-RSE (או Residual standard error), שגם הוא נמצא בתחתית סיכום מודל הרגרסיה. ה-RSE מחושב על ידי הנוסחה הבאה:

\[\text{RSE} = \sqrt{\frac{\text{RSS}}{n-2}} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2}\]

הנוסחה עבור $R^2$ נתונה על ידי:

\[R^2 = 1 - \frac{\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2}{\sum_{i=1}^n\left(y_i-\bar{y}_i\right)^2} = 1 - \frac{\text{RSS}}{\text{TSS}}\]

לא ניכנס לעומק התיאוריה של מודל הרגרסיה הלינארית, אך לקורא המעוניין ניתן להעמיק בספר הבא (פרק 3):

<div style="direction:ltr; text-align:left;">Gareth J., Witten D., Hastie T., and Tibshirani R., An Introduction to Statistical Learning with Applications in R, <i>Springer</i>, 7th printing, 2017. Online access: <a href="http://www.statlearning.com">www.statlearning.com</a>, (feteched October 2018).</div>

במישור המעשי, מודל של רגרסיה לינארית מתאים כדי לחזות משתנה מטרה רציף (שמקבל ערכים בטווח ערכים מסוים), אך במקרים מסוימים עשוי להתאים גם לבעיות סיווג או למשתנה מטרה אורדינלי (בדיד עם סט ערכים סופי שיש ביניהם יחס סדר).

### משתנים קטגוריים ברגרסיה

בדוגמה לעיל עסקנו במשתנים רציפים בלבד. אך לעיתים ישנם משתנים קטגוריאליים שאנו רוצים להשתמש בהם במסגרת מודל החיזוי, למשל השפעת מגדר על גובה ההכנסה. מגדר הוא משתנה המקבל אחת משתי קטגוריות. כיצד ניתן להשתמש בו במודל רגרסיה?

במודל הרגרסיה הוא יבוא לידי ביטוי כתוספת בעבור ערך מסוים.

\[\text{Salary} = \beta_0 + \beta_{\text{f}}\cdot X_{\text{female}}\]

אם התצפית מתייחסת לנקבה אז ערך המשכורת ישתנה בתוספת של $\beta_{\text{f}}$. הרמה הנומינלית תהיה (עבור גברים) $\beta_0$.

***

#### שאלה למחשבה

   1. איך נטפל במשתנה שיש לו כמה ערכים בדידים?
      a. משתנה אורדינלי (לדוגמה רמת שכר)
      b. משתנה פקטור (לדוגמה מצב משפחתי)

***

### הכללות למודל לא לינארי

בפונקציה `lm` ניתן להכניס יחסית בקלות הכללות למודל הלינארי, על ידי שינוי מתאים במשתנים. לדוגמה, הפקודה הבאה תכניס משתנים חדשים עם יחסים של מכפלות או ריבוע. עם זאת יש להיזהר מ"קללת המימד" עליה הסברנו בפרק קודם.
בדוגמה הבאה הכנסנו גם את משתנה הפקטור Species, את האינטראקציה בין אורך ורוחב של עלי כותרת, ואת האורך של העלים בריבוע.

```{r non-linear regression}

iris_nonlm <- lm(data = iris %>% mutate(sqaured.Length = Sepal.Length^2),
                 formula = 
                   Sepal.Width ~ 
                   Sepal.Length + Petal.Length + Petal.Width + 
                   sqaured.Length + Petal.Length*Petal.Width + 
                   factor(Species))
summary(iris_nonlm)

```

***

### תרגיל (שהוא גם תחרות)

בתרגיל זה תתאימו מודל לחיזוי מחירם של יהלומים.

   1. מאגר הנתונים diamonds נטען אוטומטית ב-R. הציצו בנתונים ובהסבר עליהם.
```{r glimpse mtcars}

glimpse(diamonds)
# Use ?diamonds to see the documentation of the database
# or click F1 on diamonds

# Set a consistent test set among the groups - USE THIS SET AS A TRAIN SET OF 80%
train_set_idx <- round(seq(1, NROW(diamonds), length.out = (NROW(diamonds)*0.8)))

```

   2. חלקו את הנתונים באמצעות הוקטור train_set_idx לסט אימון וסט בחינה.
   3. בנו את המודל הטוב ביותר שאתם יכולים על ה-train set. השתדלו לבנות את המודל הטוב ביותר עם מספר המשתנים הקטן ביותר האפשרי. אתם יכולים להשוות בין מודלים על ידי הסתכלות על רמת המובהקות של המשתנים, ועל ה-R^2 של המודלים.
   4. חשבו, האם יש מקום לפצל את העבודה למספר מודלי רגרסיה לפי משתנים מסוימים?
   5. לאחר שבחרתם מודל או מודלים, בדקו את ממוצע ריבועי השגיאות על ה-test set.
      a. כדי לבדוק את סכום ריבוע השגיאות, השתמשו בפונקציה `predict` באופן הבא:
      
```
predict(linear_model, newdata,...)

```

   6. כדי לחשב את ממוצע ריבועי השגיאות השתמשו ב-mutate כדי לחשב את הוקטור הבא  (price-predicted_price)^2, ואז חשבו ממוצע ערכי הוקטור שקיבלתם.

הקבוצה המנצחת היא זו שהגיעה לשגיאה הנמוכה ביותר.


```{r diamonds stepwise regression, include=FALSE}

lm_full_diamonds <- lm(data = diamonds[train_set_idx,],
              formula = paste0("price ~ ", paste(names(diamonds)[c(1:6, 8:10)], collapse = " + ")))
summary(lm_full_diamonds)

step_wise_model_diamonds <- MASS::stepAIC(lm_full_diamonds, direction = "backward", trace = TRUE)
summary(step_wise_model_diamonds)

diamonds_predictions <- predict(step_wise_model_diamonds,
                                newdata = diamonds %>%
                                  filter(!(seq_along(price) %in% train_set_idx))) %>%
  tibble(price_prediction = .) %>%
  bind_cols(price = diamonds$price[-train_set_idx])

# RMSE
sqrt(mean((diamonds_predictions$price-diamonds_predictions$price_prediction)^2))

# MAE
mean(abs(diamonds_predictions$price-diamonds_predictions$price_prediction))

# ME
mean((diamonds_predictions$price-diamonds_predictions$price_prediction))

# The distribution of errors: see if the error is uniform as a function of price.
# Can you spot a bias? Would you say the model perform well?
ggplot(diamonds_predictions, aes(x = price, y = (price_prediction - price))) + geom_point()

diamonds_predictions %>% 
  gather(type_price, value) %>%
  ggplot(data = ., aes(value, x = type_price)) + geom_boxplot()

```


***

### בחירות משתנים

חלק חשוב בבניית מודלים (ולמעשה אף בהכנת ה-data לפני בניית המודלים), הוא צמצום מימדים, ובחירת משתנים (Variable selection, dimension reduction). באמצעות תהליך "צעדים" ניתן להנחות את המחשב לבחור רגרסיה המכילה תת-קבוצה של המשתנים. נמחיש פקודה העוזרת לצמצם את מודל הרגרסיה למשתנים המשמעותיים ביותר:

```{r stepwise}

step_wise_model <- MASS::stepAIC(iris_nonlm, direction = "both", trace = TRUE)
summary(step_wise_model)

```

דרך נוספת לצמצם מימדים (כאשר מדובר במשתנים רציפים) היא באמצעות ניתוח גורמים עיקריים Principle Component Analysis. ככלל, לפני שאנחנו מתאימים מודל, נרצה לוודא שהמשתנים אינם בעלי קורלציה אחד לשני (כדי לא להכניס משתנים מיותרים, וכדי לא ליצור אפקטים מטעים במודלים). האלגוריתם של PCA מחלץ מהנתונים "וקטורים" לפי סדר יורד שבו בכל וקטור השונות היא "הגבוהה ביותר האפשרית".

כך הנתונים מתפרקים לגורמים המסבירים את השונות, וניתן לבחור תת קבוצה של וקטורים. החיסרון הוא שלא תמיד הפירוק ניתן להסבר בצורה מיידית.

```{r pca example, message=FALSE}

no_show_appointments <- read_csv("data-files/Medical_Appointments_No_Shows_KaggleV2-May-2016.csv") %>% 
  select(Age, Scholarship:SMS_received) %>%
  mutate(is_train = runif(NROW(Age)) <= 0.8)



pca1 <- prcomp(no_show_appointments %>% select(-Age) %>% filter(is_train))
pca1
summary(pca1)

# to retreive the variables use
pca1_use <- predict(pca1, newdata = no_show_appointments %>% select(-Age) %>% filter(!is_train))
glimpse(pca1_use)
# now you can continue to work with pca1_use as your data set

# If you add age, you will see that it becomes the most significant element (PC1)
pca_with_age <- prcomp(no_show_appointments)
pca_with_age
summary(pca_with_age)

```

### חולשות הרגרסיה הלינארית

לרגרסיה הלינארית ישנן מספר חולשות שכדאי להיות מודעים אליהם:
   
   * הנחה פשטנית למדי של לינאריות הנתונים.
   * הנחת שונות קבועה (הטרוסקדסטיות) - השונות של השגיאה הסטטיסטית קבועה בין רמות שונות של המשתנים המסבירים
   * קורלציה בין משתנים מסבירים תגרום לתופעות לא רצויות
   * מושפעת מאוד מערכי קיצון. למשל, ניקח את הנתונים של iris_setosa ונוסיף להם ערך יחיד אבל קיצוני במיוחד.
   
```{r illustration of outliers}

iris_setosa_outlier <- iris_setosa %>%
  add_row(Sepal.Length = 5.1, Sepal.Width = 35, Petal.Length = 1.4, Petal.Width = 0.2, Species = "setosa")

ggplot(iris_setosa_outlier, aes(x = Sepal.Length, y = Sepal.Width)) + 
  geom_point() + stat_smooth(method = "lm", se = FALSE,
                             linetype = "dashed") + 
  stat_smooth(inherit.aes = FALSE,
              method = "lm", 
              data = iris_setosa, aes(x = Sepal.Length, y = Sepal.Width), 
              se = FALSE) + 
  coord_cartesian(ylim = c(2, 4.5)) + 
  ggtitle("The influence of a single outlier on the regression model\nChart is zoomed-in, i.e., the outlier (5.1, 35) is not visible in the chart")

```

הוספה של תצפית אחת חריגה (5.1, 35) "זרקה" את מודל הרגרסיה בצורה קיצונית. לכן, לפני הרצת מודל רגרסיה (ובאופן כללי מודלים של חיזוי) מומלץ לבדוק ערכים חסרים ולטפל בהם או להחריג אותם מהניתוח.

## הקשר שבין רגרסיה לינארית לבין KNN

בפרק הקודם עסקנו באלגוריתם KNN. המחשנו את השימוש ב-KNN לצורך בעיות סיווג, אך ניתן להפעיל אותו גם לצורך חיזוי משתנים רציפים, מעין "רגרסיה מבוססת שכנים" כאשר הערך שינתן לתצפית חדשה יבוסס על הערך הממוצע של השכנים שלה.

\[\hat{y}_X=\sum_{i\in\mathcal{N}_k(X)}y_i\]

***

### תרגיל - רגרסיה לינארית

הקובץ NYC_payroll_sample.csv הוא מדגם מתוך נתוני המשכורות של עובדי עיריית ניו-יורק.
[https://data.cityofnewyork.us/widgets/k397-673e](קישור לקובץ המלא כ-400MB).
לצורך התרגיל, ניתן לעבוד עם המדגם עצמו (כ-70,000 שורות נתונים מתוך 2.7 מיליון).
קובץ המדגם זמין בתיקייה וגם בעמוד ה-github תחת שם הקובץ NYC_payroll_sample.csv

תיאור הקובץ והמשתנים שבו נמצא בקובץ: Open-Data-Dictionary-Citywide_Payroll.FINAL.xlsx

עליכם לטעון את הקובץ ולהשתמש במשתנים שבו לצורך חיזוי רמת השכר של עובד חדש המתקבל לעבודה בעיריית ניו-יורק. להלן כמה נקודות למחשבה:

   * בדקו: האם ישנם נתונים "חריגים" שיש להוציא מהניתוח? נניח רמות שכר קיצוניות גבוהות או נמוכות. האם יש עובדים שצריך להחריג?
   * באילו משתנים אתם הולכים להשתמש? איזה משתנים רציפים, איזה משתנים הם פקטורים?
   * * האם ישנם משתנים שנדרש לבצע עליהם טרנספורמציה ולהמיר אותם לפורמט אחר?
   * האם נדרשת עבודת הכנה על משתנים מסוג character? אתם יכולים להיעזר בקובץ [שבקישור](https://www.rstudio.com/resources/cheatsheets/#stringr)
   * לעבודה על משתנים מסוג תאריך ניתן להיעזר בקובץ [שבקישור הזה](https://www.rstudio.com/resources/cheatsheets/#lubridate)
   * האם ישנם ערכים כפולים או לא מדוייקים למשתנים מסוימים? (לדוגמה אותיות קטנות/גדולות)
   * כיצד תתייחסו למשתנה "שנה פיסקלית"?
   * חשוב! לפני בניית המודל השתמשו בחלוקה ל-Train/Test כדי שתוכלו לבדוק את מידת הדיוק שלכם על ה-Test set. באיזה מדד תשתמשו כדי לבדוק את רמת הדיוק?
   * כדי לחשב ערכי משכורת על ה-Test set, השתמשו בפונקציית `predict`



```{r regression exercise NYC payroll, include=FALSE}

library(lubridate)

payroll_raw <- read_csv("data-files/NYC_payroll_sample.csv")
payroll_sample <- payroll_raw %>%
  mutate(tenure = 
           `Fiscal Year` - year(as_date(x = `Agency Start Date`, format = "%m/%d/%Y", tz = "EST"))) %>% 
  filter(`Pay Basis` == "per Annum") %>% 
  mutate(is_train = runif(n = length(seq_along(`Fiscal Year`))) <= 0.8)

payroll_lm <- lm(data = payroll_sample %>% filter(is_train),
                 formula = `Base Salary` ~ tenure + `Fiscal Year`)

payroll_predict <- predict(payroll_lm, newdata = payroll_sample %>% filter(!is_train))

# plot errors as a function of wage level
payroll_test <- payroll_sample %>%
  filter(!is_train) %>%
  mutate(prediction = payroll_predict)

ggplot(payroll_test, aes(x = `Base Salary`, y = abs(`Base Salary` - prediction))) + geom_point()

rmse <- payroll_test %>%
  mutate(sq_err = (`Base Salary` - prediction)) %>%
  summarise(sum(sq_err, na.rm = T))
```

***

### בפרק הבא

בפרק הבא נדון בסוג אחר של רגרסיה - רגרסיה לוגיסטית. בהרבה מובנים רגרסיה לוגיסטית דומה לרגרסיה לינארית, אך היא מתאימה הרבה יותר לסיווג לקטגוריות (או לאבחון כישלון/הצלחה) בהשוואה לרגרסיה לינארית.